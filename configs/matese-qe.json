{
  "reference_less": true,
  "transformer_model_name": "xlm-roberta-large",
  "encoder_layers": 2,
  "encoder_attention_heads": 8,
  "dropout_rate": 0.1,
  "use_last_n_layers": 4
}